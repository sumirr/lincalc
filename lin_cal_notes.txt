
#btnPretty

the fractional conversion button a/b is not working correctly.
At this point, I am just going to remove it.
name btnPretty

Sender: btnPretty
Signal: clicked
Receiver: MainWindow
Slot: pretty()


# unosorted

        # activate python
        PS> venv\Scripts\activate

        # matrix calc



# Vectors
Vectors can be subjected to several operations, including:

Vector addition and subtraction
        The rules for vector addition and subtraction are:
        Vector addition: To add two vectors, you add their corresponding components. For example, if we have two vectors a = (a1, a2, a3) and b = (b1, b2, b3), their sum a + b is (a1 + b1, a2 + b2, a3 + b3).
        Vector subtraction: To subtract two vectors, you subtract their corresponding components. For example, if we have two vectors a = (a1, a2, a3) and b = (b1, b2, b3), their difference a - b is (a1 - b1, a2 - b2, a3 - b3).
        In both cases, the resulting vector has the same number of components as the original vectors. Additionally, the order of the vectors being added or subtracted does not matter, so a + b = b + a and a - b ≠ b - a.
        

Scalar multiplication and division
Dot product (also known as inner product)
Cross product (also known as vector product)
Projection
Reflection
Magnitude (also known as length or norm)
Unit vector
Angle between vectors
These operations are fundamental in vector calculus and are used in various fields such as physics, engineering, computer graphics, and machine learning, among others.


- area of a triangle
- sides of parallelogram?
## size chec
No, it is not true for all of these operations.

Cross Product: Cross product can only be calculated for two vectors in three-dimensional space. The two vectors need to be of the same size, i.e., they should have three elements each.

Vector Addition and Subtraction: Vector addition and subtraction can be performed on vectors of different sizes, but the vectors should have the same dimension. For example, a 2-dimensional vector can be added to another 2-dimensional vector, but it cannot be added to a 3-dimensional vector.

Projection: Projection can be calculated for vectors of any size or dimension. The projection of a vector onto another vector depends only on the direction of the vectors, not on their length.

Reflection: Reflection can also be calculated for vectors of any size or dimension. The reflection of a vector about a plane is defined by the angle between the vector and the normal to the plane.


## dot_product tests
Here are some possible tests you can perform to check if a dot product is working correctly:

Test the commutative property: For two vectors A and B, compute A·B and B·A. If the dot product is working correctly, you should get the same result for both calculations.

Test the distributive property: For three vectors A, B, and C, compute A·(B+C) and A·B + A·C. If the dot product is working correctly, you should get the same result for both calculations.

Test the associative property with scalar multiplication: For a scalar a and two vectors A and B, compute a(A·B), (aA)·B, and A·(aB). If the dot product is working correctly, you should get the same result for all three calculations.

Test the dot product of a vector with itself: For a vector A, compute A·A and compare it to the square of the magnitude of A (i.e., |A|^2). If the dot product is working correctly, you should get the same result for both calculations.

Test the angle between two vectors: For two vectors A and B, compute the angle between them using the formula cosθ = (A·B) / (|A| |B|). Then, use a calculator or a tool like Python's math library to compute the actual angle between the vectors (e.g., using the acos function). If the dot product is working correctly, the computed angle should be close to the actual angle.

Test for orthogonal vectors: For two vectors A and B that are supposed to be orthogonal (i.e., perpendicular) to each other, compute A·B. If the dot product is working correctly, the result should be very close to zero.

## dimensions
	No, a vector can have multiple dimensions. A one-dimensional vector is sometimes called a "scalar" because it only has one component, but a vector can have any number of components. For example, a two-dimensional vector has two components (often called x and y), and a three-dimensional vector has three components (often called x, y, and z). In general, an n-dimensional vector has n components.
	
	
	No, that is not necessarily true. A vector can have multiple rows and columns, depending on the context in which it is used. In linear algebra, a vector is typically represented as a column vector or a row vector.
	
	A column vector is a matrix with a single column, which can have multiple rows, representing a set of values in a vertical orientation. For example:
	
	Copy code
	[1]
	[2]
	[3]
	A row vector, on the other hand, is a matrix with a single row, which can have multiple columns, representing a set of values in a horizontal orientation. For example:
	
	csharp
	Copy code
	[1, 2, 3]
	So, a vector can have multiple rows or columns, but it is typically represented as either a column vector or a row vector.
	

# notes:
https://docs.sympy.org/latest/modules/matrices/matrices.html

## permanent
The permanent of A, denoted as perm(A), is calculated by taking the sum of all possible products of the elements of A, where each product consists of exactly one element from each row and each column. So we have:

markdown
Copy code
perm(A) = 1*5*9 + 1*6*8 + 2*4*9 + 2*6*7 + 3*4*8 + 3*5*7
        = 45 + 48 + 72 + 84 + 96 + 105
        = 450
Therefore, the permanent of matrix A is 450.

# test matrices
1,2,3
4,5,6
7,8,9

1.2,4.5,5.3
4.5,2.5,6.4
9.5,3.5,8.5

- symmetric A = A^T
1,2,3
2,4,5
3,5,6

- skew_symmetric A^T = -A
0,2,-1
-2,0,4
1,-4,0

- jordan form

0,1,0,0
11,6,-4,-4
22,15,-8,-9
-3,-2,1,2

- fractional_matrix
input_str = '''
1/2,5/6,9/18,
4/5,8/5,2/8,
5/9,4/9,4/5
'''
'''
1/2,5/6,9/18
4/5,8/5,2/8
5/9,4/9,4/5
'''

1. Jordan Form: 

⎡-1  1   0  0⎤
⎢0   -1  0  0⎥
⎢0   0   1  1⎥
⎣0   0   0  1⎦



# changes
[sympy]pretty.py
        -line: 706 modified the vsep from 1 to 0
         # h- and v- spacers
        hsep = 2
        vsep = 0
        

# other functionality
> print to latex
> print to mathml

> [done] swap button 
> orthogonal matrix
> [no] identity matrix / elementary matrix
> !incompatible matrix sizes
> [done]  adjugate, conjugate, normal


# to do
[done][todo] strech factors all set to 0. Fixed layout preventing scaling.
        - added QVboxLayout to visual elements to induce scaling
[todo] add in a import function
[todo] when you add in a number in to the power to box, then press enter - it should do the calculation and not wait for the button press
[done][todo] random generated matrices have two labels. The label should display 2 x 2, but currently says 2.
[later][todo] add a unified line showing the input + the output. cofactor(matrix) = (output)
[done][todo] add a insert into A, insert into B button for the output
[todo] perhaps add a clear button to edtOutput
[possibly] might change the text alignment from left to right.
        #self.ui.edtOutput.setAlignment(Qt.AlignRight)
[todo] allow the user to the precision, when returning floating points numbers
[todo] add a clear button for the main output
[done][todo] sort out issue with generating one dimensional matrix


# size of matrice
The size of a matrix is usually measured in terms of its number of rows and columns. The minimum and maximum sizes for a matrix can vary depending on the context and the specific problem being studied. Here are some common examples of minimum and maximum sizes for matrices:

Minimum size: The smallest possible matrix is a 1x1 matrix, which contains a single element. Matrices of this size are rarely used in practice, except as a theoretical construct.

Small size: Matrices with sizes up to 5x5 or 10x10 are relatively small and are commonly used in introductory courses in linear algebra or other areas of mathematics. These matrices are often used to introduce basic matrix operations such as addition, subtraction, multiplication, and inversion.

Medium size: Matrices with sizes up to 100x100 or 1000x1000 are considered medium-sized and are commonly used in applications such as image processing, data analysis, and numerical simulations. These matrices can represent large data sets or complex systems of equations.

Large size: Matrices with sizes greater than 1000x1000 are considered large and are used in applications such as machine learning, computational fluid dynamics, and weather modeling. These matrices can represent very large data sets or very complex systems of equations.

# unisa linear algebra modules
Here is a list of UNISA modules that cover Linear Algebra:

MAT2611 - Linear Algebra A
MAT2612 - Linear Algebra B
MAT3701 - Linear Algebra C
MAT3702 - Advanced Linear Algebra A
MAT3703 - Advanced Linear Algebra B
Note that some of these modules may have prerequisites or co-requisites, so it is important to check the requirements before enrolling in a module

⎡1  0  -1⎤
⎢        ⎥
⎢0  1  2 ⎥
⎢        ⎥
⎣0  0  0 ⎦




# issues


[done][issue] when generating random matrices, program cant generate a random matrice of size 1 for some reason.
[done][issue] it cant do fractional calculations. but then how are fractional matrices.
        [chatgpt]Matrices with fractions are quite common in many areas of mathematics, such as linear algebra, algebraic geometry, and number theory. Matrices with 
        fractions are often used in applications that involve computations with rational numbers, such as solving linear systems of equations or computing determinants
        .
        In linear algebra, for example, the coefficients of the equations in a system of linear equations are often rational numbers, and these coefficients are 
        usually organized into a matrix. When the matrix has entries that are fractions, the resulting computations may involve fractions as well.

        In algebraic geometry, matrices with entries in a field of fractions are often used to represent projective varieties. In this context, the entries of the 
        matrix are homogeneous polynomials, which are often expressed as quotients of polynomials.

        In number theory, matrices with entries in a field of fractions are used to study algebraic number fields and their associated rings of integers. In this 
        context, the entries of the matrix are algebraic numbers, which are often expressed as fractions of polynomials with integer coefficients.

        Overall, matrices with fractions are quite common in mathematics, and their use is often motivated by practical applications and theoretical considerations




## insertion failing
[done][major ]- issue with the insertion, its loosing its contents
        - need to add a pretty button for fracitonal calculations


## inverse and transpose()
[done]in the inverse and transpose, the output is explicitly set to float.

```python
output_formatted = output.evalf(2)
            self.matrix_type = "float"
```

This is because we had an issue, when converting a matrice to inverse, it often results in fractions.

 The fractional check is done in validate_matrix which only gets called when a matrix is inserted manually, or generated randomly.

 So the program, does not know that the inverse matrix is actually matrice, as the validation does get called. So when it outputs, it uses the return_numpy_array method - which unfortunately treats the fractional matrix as a integer matrix, and therefore inserts incorrectly.

 Also theres the issue of inverse being displayed as 1/3 - and this might induce the user to also enter in a fractional matrix in the format, as oppose to 0.34

 A simalar situaion occurs in the transpose, we also set the self.matrix_type explicitly as oppose to dynamically,



## eigenvalues_
In [18]: A.eigenvals()
Out[18]: 
⎧                     ___________                                                                                        ________
⎪    ⎛  1   √3⋅ⅈ⎞    ╱ √137   13                 2                                    2                 ⎛  1   √3⋅ⅈ⎞    ╱ √137   
⎪1 + ⎜- ─ - ────⎟⋅3 ╱  ──── + ──  + ────────────────────────────: 1, 1 + ──────────────────────────── + ⎜- ─ + ────⎟⋅3 ╱  ──── + 
⎨    ⎝  2    2  ⎠ ╲╱    2     2                      ___________                          ___________   ⎝  2    2  ⎠ ╲╱    2     
⎪                                   ⎛  1   √3⋅ⅈ⎞    ╱ √137   13          ⎛  1   √3⋅ⅈ⎞    ╱ √137   13                             
⎪                                   ⎜- ─ - ────⎟⋅3 ╱  ──── + ──          ⎜- ─ + ────⎟⋅3 ╱  ──── + ──                             
⎩                                   ⎝  2    2  ⎠ ╲╱    2     2           ⎝  2    2  ⎠ ╲╱    2     2                              

___                               ___________   ⎫
13             2                 ╱ √137   13    ⎪
── : 1, ─────────────── + 1 + 3 ╱  ──── + ── : 1⎪
2           ___________       ╲╱    2     2     ⎬
           ╱ √137   13                          ⎪
        3 ╱  ──── + ──                          ⎪
        ╲╱    2     2                           ⎭

In [19]: A.eigenvals().simplify()


# commands to push
commands to push to git repository


zar@omkara:~/code/qt/test2$ git init

zar@omkara:~/code/qt/test2$ git add .
zar@omkara:~/code/qt/test2$ git commit -m "initial commit"
[master (root-commit) a9c65be] initial commit
 9 files changed, 2001 insertions(+)
 create mode 100644 .gitignore
 create mode 100644 dark.css
 create mode 100644 dark.qss
 create mode 100644 form.py
 create mode 100644 form.ui
 create mode 100644 mainwindow.py
 create mode 100644 test2.pyproject
 create mode 100644 test2.pyproject.user
 create mode 100644 ui_form.py
zar@omkara:~/code/qt/test2$ git branch -M main
zar@omkara:~/code/qt/test2$ git remote add origin git@github.com:mesoclever/linear_algebra_calc.git
zar@omkara:~/code/qt/test2$ git push -u origin main


# data types
## cofactor
        In [87]: A.cofactor_matrix()
        Out[87]: 
        ⎡-3  12   -8⎤
        ⎢           ⎥
        ⎢6   -15  8 ⎥
        ⎢           ⎥
        ⎣-3   6   -3⎦

        In [88]: type(A.cofactor_matrix())
        Out[88]: sympy.matrices.dense.MutableDenseMatrix

## determinant
        In [89]: type(A.det())
        Out[89]: sympy.core.numbers.Integer

## eigenvalues
        In [90]: type(A.eigenvals())
        Out[90]: dict

## trace
        In [91]: type(A.trace())
        Out[91]: sympy.core.numbers.Integer

## transpose
        In [92]: type(A.transpose())
        Out[92]: sympy.matrices.dense.MutableDenseMatrix
## rank
        In [10]: type(A.rank())
        Out[10]: int

## inverse
        In [12]: type(A.inv())
        Out[12]: sympy.matrices.dense.MutableDenseMatrix

## power of
        In [13]: type(A.applyfunc(lambda e: e**2))
        Out[13]: sympy.matrices.dense.MutableDenseMatrix

## multiply by

        In [14]: type(A * 5)
        Out[14]: sympy.matrices.dense.MutableDenseMatrix

## rref
        sympy.matrices.dense.MutableDenseMatrix        

# to add
https://docs.sympy.org/latest/modules/matrices/common.html

In [4]: A.adjoint()
Out[4]: 
Matrix([
[1, 3, 4],
[2, 4, 5],
[3, 5, 2]])


In [6]: A.conjugate()
Out[6]: 
Matrix([
[1, 2, 3],
[3, 4, 5],
[4, 5, 2]])



## is:
        - is_lower
        - is_upper
        - is_diagonal
        - is_hermitian
        - is_lower_hessenberg
        - is_square
        - is_symmetric





# definitions

what is a hermitian matrix
A Hermitian matrix is a square matrix that is equal to its own conjugate transpose. In other words, if A is a Hermitian matrix, then A is equal to the complex conjugate of its transpose, denoted by A^H:

A = A^H

Where A^H is the conjugate transpose of A, obtained by taking the transpose of A and then conjugating each element.

The diagonal entries of a Hermitian matrix are always real numbers, and the off-diagonal entries are complex conjugates of each other. Hermitian matrices are also known as self-adjoint matrices, since they satisfy the same condition as a self-adjoint linear operator in a complex inner product space.

Hermitian matrices have many important properties in linear algebra and in physics, including that they have real eigenvalues and orthogonal eigenvectors, and that they can be diagonalized by a unitary matrix.



## list of functions

Certainly! Here are some common functions that can be performed on matrices:

Addition and subtraction: The sum or difference of two matrices of the same dimensions is obtained by adding or subtracting the corresponding entries.

Scalar multiplication: Multiplying a matrix by a scalar multiplies each entry of the matrix by the scalar.

Matrix multiplication: The product of two matrices A and B is obtained by multiplying the rows of A by the columns of B.

Transpose: The transpose of a matrix A is obtained by interchanging its rows and columns.

Determinant: The determinant of a square matrix A is a scalar value that can be computed using various methods and provides important information about the matrix, such as whether it is invertible or not.

Inverse: The inverse of a square matrix A is another square matrix A^-1, such that AA^-1 = A^-1A = I, where I is the identity matrix.

Trace: The trace of a square matrix A is the sum of its diagonal entries.

Eigenvalues and eigenvectors: Eigenvalues and eigenvectors provide important information about a matrix, such as how it scales certain vectors.

Rank: The rank of a matrix is the dimension of the subspace spanned by its columns or rows.

Diagonalization: A diagonalizable matrix can be expressed in terms of its eigenvectors and eigenvalues.

Adjoint: The adjoint or adjugate of a square matrix A is the transpose of its cofactor matrix, obtained by replacing each entry of A by its corresponding cofactor and then taking the transpose.

Cofactor: The cofactor of an element of a square matrix is the signed determinant of the submatrix obtained by deleting the row and column containing that element.

Singular Value Decomposition (SVD): A decomposition of a matrix into a product of three matrices: A = USV^T, where U and V are orthogonal matrices, and S is a diagonal matrix containing the singular values of A.

QR Decomposition: A decomposition of a matrix into a product of two matrices: A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix.

Cholesky Decomposition: A decomposition of a positive definite matrix into a product of a lower triangular matrix and its conjugate transpose.

LU Decomposition: A decomposition of a square matrix into a product of a lower triangular matrix and an upper triangular matrix.

Schur Decomposition: A decomposition of a square matrix into a triangular matrix and an orthogonal matrix.

Jordan Normal Form: A canonical form of a matrix that is useful for understanding the behavior of a matrix under repeated applications.

Kronecker Product: A product of two matrices that produces a larger matrix by taking each element of one matrix and multiplying it by the other matrix.

Hadamard Product: An element-wise product of two matrices of the same dimensions.


